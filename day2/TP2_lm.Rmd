---
title: "TP2 : Linear Regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Basic linear regression

Let $x_1$ and $x_2$ be 2 explanatory variables and $y$ the dependent variable. $\epsilon$ (espilon) the error of  prediction (supposed to be a random variable). We modelise $y = b_0 + b_1x_1 + b_2x_2 + \epsilon$. The question is how to find the best coefficients $b_0$, $b_1$ and $b_2$ to minimize the error of modelisation : $(y - \hat y)^2$ with $\hat y$ the calculated estimation of $y$ by $\hat y = b_0 + b_1x_1 + b_2x_2$.

The function `lm` of R makes it possible to calculate the linear regression of a numerical dependent variable as a function of explanatory variables (here $y = f(x_1, x_2) + \epsilon$).

#### 1 - Use `lm` on a simple example

You can use the help panel (or press F1 with the word selection) to fetch R's help documentation.

Here the formula is reduced to $y = b_0 + b_1x + \epsilon$.

```{r}
set.seed(123)
x <- c(4, 6, 3, 5, 1, 9)
x <- sort(x)
y <- 2 * x + 1 + rnorm(6, 0, 0.3)

plot(x, y, type = "b")
```


```{r}
fit <- lm(formula = y ~ x)
fit
```

L'estimation linéaire obtenue est $y = 0.7956 + 2.0725 \times x$.

```{r}
# plot(fit)

plot(x, y, type = 'b')
lines(x, fit$coefficients[1] + fit$coefficients[2]*x, type = 'b', pch = 2, lty = 2, col=2)
```


```{r}
library(ggplot2)

ggplot(data = data.frame(x, y), aes(x, y)) +
        geom_point() +
        geom_smooth(method = "lm")
```


#### 2 - With a data frame

```{r}
fr <- data.frame(x = c(4, 6, 3, 5, 1, 9), y = c(8, 11, 5, 10, 3, 17), z = c("a", "a", "b", "a", "b", "a"))

plot(fr$x, fr$y)
```


```{r}
fit2 <- lm(y ~ x + z , data = fr)
fit2
```


#### 3 - Draw the graph with the line using plot function

--> cf avant
```{r}

```


#### 4 - Interpret the object, of class "lm"

cf formule
```{r}

```

#### 5 - Give the confidence interval on the coefficients, with alpha = 0.95

On utilise l'estimation de l'interval (Gaussien) de confiance IC = [moyenne - 3 écart type, moyenne - 3 écart type] 

```{r}
summary(fit)
# sum_fit <- summary(fit)
# names(fit)
# names(sum_fit)
```


```{r}

```


#### 6 - Prediction of values
The predict function returns a vector of the predicted values (names of the explanatory variables must be the same in the data frame).

```{r}

```


#### 7 - Plot the confidence intervals (of the predicted values)

```{r}
y_pred <- predict.lm(fit, newdata = data.frame(x), se.fit = TRUE )
# plot(x, y_pred)
# sigma <- sd()
# ic <- c(mean(y))
# plot(y_pred, se.fit = TRUE)
# plot(y_pred)
new <- data.frame(x)
# predict(lm(y ~ x), new, se.fit = TRUE)
# pred.w.plim <- predict(lm(y ~ x), new, interval = "prediction")
pred.w.clim <- predict(lm(y ~ x), new, interval = "confidence")
matplot(new$x, cbind(pred.w.clim),
        lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")
```


#### 8 - Goodness of the model

```{r}
summary(fit)
```

Le R² (R-squared) est de 0.9987 ce qui réflète une très bonne modélisation.



Calculate the mean square error and comment on its meaning.

RMSE:

```{r}
sqrt( mean( (y_pred$fit - y)**2 ) )
```

#### 9 - Plot the coefficients interval of confidence

```{r}

```


## Using the diamond data

A common method of machine learning is to use a train sample to train the algorithm and then to evaluate the goodness of the model on a test sample.

Use the following train and test sample from the `diamonds` dataset to predict the `price` of a diamond in function of its `carat` and `clarity` value.


```{r}
# to install pacakges
# install.packages("ggplot2")
# install.packages("dplyr")

# --- test if installed
library(ggplot2)
library(dplyr)

diamonds <- ggplot2::diamonds
str(diamonds)
```

```{r}
ggplot(data = diamonds, mapping = aes(x = color, y = price, color = carat)) +
        geom_point(alpha = .2)
# carat
```


```{r}

```


```{r}
diamonds %>% 
        ggplot(data = ., mapping = aes(x = color, y = price)) +
        # geom_point()
        geom_boxplot()
# cut
```


```{r}
# plot(head(diamonds, 1e3 ) )
# plot(diamonds)
```

### Modelisation

```{r}
set.seed(123)
sample_index <- sample(x = 1:nrow(diamonds), size = floor(0.8*nrow(diamonds)), replace = FALSE)
train <- diamonds[sample_index , ]
# ytrain <- diamonds[sample_index , ]$price
test <- diamonds[ - sample_index , -7]
ytest <- diamonds[ - sample_index , ]$price
# str(train)
# str(test)

dim(train)
dim(test)
```

#### LM

Effectuer une régression linéaire du prix en fonction de la variable `carat` :

**Sur l'échantillon d'apprentissage**

```{r}
fit_d <- lm(price ~ carat + ... , data = train)
fit_d
```

La formule obtenue : $Price = -2260 + 7761 \times Carat$ signifie : "pour chaque nouvelle unité de carat on ajoute 7761 $(€ ?) au prix d'un diamand. 

Représentation graphique :

```{r}
# plot(fit_d) # pour affiner l'analyse stat
train %>% 
        ggplot(data = ., mapping = (aes(carat, price))) +
        geom_point(alpha = .1) +
        geom_smooth(method = "lm")
```

On trouve un $R^2$ de 0.8499 pour cette modélisation :

```{r}
summary(fit_d)
```

Ajouter plus de variables à la régression (formule : `~ carat + nouvelle_variable`)

```{r}
fit_d_all <- lm(price ~ . , data = train)
fit_d_all
```

Si on augmente la dimension (avec toutes les variables) on augmente la qualité de la prévision selon le critère du $R^2$ : 0.9194

```{r}
summary(fit_d_all)
```

### Evaluer sur l'échantillon de test / faire de nouvelles prévisions

```{r}
test
pred <- predict.lm(object = fit_d, newdata = test)
head(pred)
head(ytest)
```

RMSE :

```{r}
sqrt( sum((pred - ytest)**2)/length(pred))

err <- pred - ytest
# err %>% .**2 %>% sum() %>% function(x){x/length(pred)} %>% sqrt()
```


```{r}
res <- data.frame(label = c(rep("pred", length(pred)), rep("real_val", length(pred))) ,value = c(pred, ytest))

# res %>% 
#         ggplot(. , aes(x = rep(1:length(pred), 2) , y = value, color = label) ) +
#         geom_point()
```

